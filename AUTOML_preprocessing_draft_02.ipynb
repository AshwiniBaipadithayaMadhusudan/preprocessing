{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno #library to visualize missing data\n",
    "import re#library for regular expression\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,Normalizer,StandardScaler,RobustScaler,LabelEncoder, OneHotEncoder \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read data to dataframe\n",
    "\n",
    "def read_file_to_df(filepath):\n",
    "    \n",
    "    df_loaded_ind=True\n",
    "    df = pd.DataFrame()#define an empty dataframe\n",
    "    \n",
    "    if filepath.endswith(\".csv\"):\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "    elif (filepath.endswith(\".xls\") | filepath.endswith(\".xlsx\")):\n",
    "        df = pd.read_excel(filepath)\n",
    "        \n",
    "    else:\n",
    "        df_loaded_ind=False\n",
    "        print('Invalid file type')\n",
    "            \n",
    "    return df_loaded_ind,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the file path for the data :SFCRIME_train.csv\n",
      "Data is successfully loaded!!\n",
      "Press enter to take a quick look\n",
      "\n",
      "                 Dates        Category                      Descript  \\\n",
      "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
      "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "\n",
      "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
      "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
      "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
      "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.425892  37.774599  \n",
      "1 -122.425892  37.774599  \n",
      "2 -122.424363  37.800414  \n",
      "3 -122.426995  37.800873  \n",
      "4 -122.438738  37.771541  \n",
      "Press enter to see the description of your data\n",
      "      count        mean       std         min         25%         50%  \\\n",
      "X  878049.0 -122.422616  0.030354 -122.513642 -122.432952 -122.416420   \n",
      "Y  878049.0   37.771020  0.456893   37.707879   37.752427   37.775421   \n",
      "\n",
      "          75%    max  \n",
      "X -122.406959 -120.5  \n",
      "Y   37.784369   90.0  \n",
      "Press enter to continue ...\n"
     ]
    }
   ],
   "source": [
    "filepath             = input('Please provide the file path for the data :')\n",
    "data_loaded_ind,data = read_file_to_df(filepath)\n",
    "if data_loaded_ind:\n",
    "    print('Data is successfully loaded!!')\n",
    "    input('Press enter to take a quick look\\n')\n",
    "    print(data.head())\n",
    "    input('Press enter to see the description of your data')\n",
    "    print(data.describe().T)\n",
    "    input('Press enter to continue ...')\n",
    "else:\n",
    "    print('Error loading dataset!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the ratio of null values in columns of dataset in descending order\n",
    "def get_null_count(df):\n",
    "   \n",
    "    na_count=df.isnull().sum().sort_values(ascending=False)/len(df)#columnwise ratio of null values\n",
    "    na_count=na_count[na_count>0]#choose only the columns whic have null values\n",
    "\n",
    "    return na_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize the null value distribution\n",
    "def visualize_null_count(df,null_count):\n",
    "\n",
    "    filtered_data = msno.nullity_filter(df,  filter='bottom',n=len(null_count))\n",
    "    msno.matrix(filtered_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.isnull().values.any():\n",
    "    null_count = get_null_count(data)\n",
    "    print('Below are the features that contain null values:\\n', null_count)\n",
    "    visualize_null_count(data,null_count)\n",
    "    plt.show()\n",
    "    input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop desired columns\n",
    "def drop_columns(df,columnlist=[]):\n",
    "    if columnlist:\n",
    "        df.drop(columnlist,axis=1,inplace=True)#Drop all the columns in the list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to drop any feature?-Y/Ny\n",
      "Enter the features you wish to drop\n",
      "Descript Resolution \n",
      "Specified features are successfully dropped...\n",
      "Existing features in the dataset are:  ['Dates', 'Category', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y']\n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drop_col_ind = (input('Would you like to drop any feature?-Y/N')).upper()\n",
    "if drop_col_ind == 'Y':\n",
    "    drop_null_column_list = []\n",
    "    print('Enter the features you wish to drop')\n",
    "    drop_null_column_list = [x for x in input().split()]\n",
    "    if drop_null_column_list:\n",
    "        data = drop_columns(data,columnlist=drop_null_column_list)\n",
    "        print('Specified features are successfully dropped...')\n",
    "        print('Existing features in the dataset are: ', data.columns.tolist())\n",
    "        input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to impute missing values\n",
    "def impute_missing(df, drop_nans=False):  \n",
    "\n",
    "    #drop the rows containing null values if drop_nans is set to True\n",
    "    if drop_nans:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        for column in df.columns.values:\n",
    "        # Replace NaNs with the median or mode of the column depending on the column type\n",
    "            try:\n",
    "                column_median = df[column].median()\n",
    "                df[column].fillna(column_median, inplace=True)\n",
    "\n",
    "            except TypeError:\n",
    "                column_mode = df[column].mode()[0]\n",
    "                df[column].fillna(column_mode, inplace=True)\n",
    "                \n",
    "    if df.isnull().values.any():\n",
    "        print('Error in replacing/dropping missing data')\n",
    "    else:\n",
    "        if drop_nans:\n",
    "            print('Missing values are successfully dropped')   \n",
    "        else:\n",
    "            print('Missing values are successfully replaced')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.isnull().values.any():\n",
    "    drop_na_rows = (input('Do you wish to drop all rows with null values?-Y/N')).upper()\n",
    "    if drop_na_rows == 'Y':\n",
    "        drop_nans = True\n",
    "\n",
    "    data = impute_missing(data, drop_nans=False)\n",
    "    input('Press enter to continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert datatype of mentioned columns to corresponding mentioned types\n",
    "def data_type_converter(df,column_names=[],to_types=[]):\n",
    "    \n",
    "    if (isinstance(column_names, list) & isinstance(to_types, list)):#check if passed item is in list format\n",
    "\n",
    "        for col,typ in zip(column_names,to_types):\n",
    "            df[col] = df[col].astype(typ) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data types of every feature are as follows:\n",
      " Dates          object\n",
      "Category       object\n",
      "DayOfWeek      object\n",
      "PdDistrict     object\n",
      "Address        object\n",
      "X             float64\n",
      "Y             float64\n",
      "dtype: object\n",
      "Would you like to change the data type of any feature?-Y/NY\n",
      "Enter the features for which you wish to change the data types\n",
      "Dates\n",
      "Enter the type to which you would like to change the data types\n",
      "datetime64[ns]\n",
      "The changed data types are as follows:\n",
      " Dates    datetime64[ns]\n",
      "dtype: object\n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "print('The data types of every feature are as follows:\\n',data.dtypes)\n",
    "type_change_ind = (input('Would you like to change the data type of any feature?-Y/N')).upper()\n",
    "if type_change_ind == 'Y':\n",
    "    cols_for_type_change=[]\n",
    "    to_type_list=[]\n",
    "    print('Enter the features for which you wish to change the data types')\n",
    "    cols_for_type_change = [x for x in input().split()]\n",
    "    print('Enter the type to which you would like to change the data types')\n",
    "    to_type_list         = [x for x in input().split()]\n",
    "    data = data_type_converter(data,column_names=cols_for_type_change,to_types=to_type_list)\n",
    "    print('The changed data types are as follows:\\n', data[cols_for_type_change].dtypes)\n",
    "    input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to separate data and target variable \n",
    "def separate_target(df_raw,target_name):\n",
    "    \n",
    "    target             = df_raw[target_name]#target variable \n",
    "    df                 = df_raw.drop(target_name,axis=1)#data without target variable\n",
    "    target_is_separate = True\n",
    "      \n",
    "    return df,target,target_is_separate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_cont_and_cat_features(df):\n",
    "    \n",
    "    categorical_features = df.select_dtypes(include=['object','bool']).columns.tolist()\n",
    "    continuous_features  = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    date_features  = [x for x in df.columns if x not in (categorical_features+continuous_features)]\n",
    "    \n",
    "    return continuous_features,categorical_features,date_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to splite timestamp/date column into sub features\n",
    "def date_splitter(df, fldname):\n",
    "    \n",
    "    #convert the specified feature to timestamp format if not already\n",
    "    fld = df[fldname] \n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, \n",
    "                                     infer_datetime_format=True)\n",
    "        \n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)#substitute the phrase 'date' by spaces in the original field name\n",
    "    \n",
    "    #obtain different sub features for the date column\n",
    "    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', \n",
    "            'Dayofyear', 'Is_month_end', 'Is_month_start', \n",
    "            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', \n",
    "            'Is_year_start'):\n",
    "        df[targ_pre+n] = getattr(fld.dt,n.lower()).apply(lambda x: int(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert date field to its Julian format\n",
    "def Juliandtconv(df,fldname):\n",
    "    \n",
    "    #convert the specified feature to timestamp format if not already\n",
    "    df[fldname]=pd.to_datetime(df[fldname],infer_datetime_format=True)\n",
    "    \n",
    "    df[fldname+'Julian'] = df[fldname].apply(lambda x: x.to_julian_date())#convert to julian format\n",
    "            \n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to concatenate data and target variable \n",
    "def concatenate_target(df,target):\n",
    "    target_is_separate=False\n",
    "    return (pd.concat([df,target],axis=1)),target_is_separate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the target featureCategory\n"
     ]
    }
   ],
   "source": [
    "target_name    = input('Please enter the target feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming date features in the data...\n",
      "The date features in the dataset are :  ['Dates']\n",
      "Please specify whether you would like to split Dates or convert it to julian format\n",
      "Enter S to split, J to convert to Julian\n",
      "\n",
      "Date tarnsformed data:\n",
      "    DayOfWeek PdDistrict                    Address           X          Y  \\\n",
      "0  Wednesday   NORTHERN         OAK ST / LAGUNA ST -122.425892  37.774599   \n",
      "1  Wednesday   NORTHERN         OAK ST / LAGUNA ST -122.425892  37.774599   \n",
      "2  Wednesday   NORTHERN  VANNESS AV / GREENWICH ST -122.424363  37.800414   \n",
      "3  Wednesday   NORTHERN   1500 Block of LOMBARD ST -122.426995  37.800873   \n",
      "4  Wednesday       PARK  100 Block of BRODERICK ST -122.438738  37.771541   \n",
      "\n",
      "    DatesJulian        Category  \n",
      "0  2.457156e+06        WARRANTS  \n",
      "1  2.457156e+06  OTHER OFFENSES  \n",
      "2  2.457156e+06  OTHER OFFENSES  \n",
      "3  2.457156e+06   LARCENY/THEFT  \n",
      "4  2.457156e+06   LARCENY/THEFT  \n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "continuous_features,categorical_features,date_features = separate_cont_and_cat_features(data)\n",
    "\n",
    "if date_features:\n",
    "    print(\"Transforming date features in the data...\")\n",
    "    print(\"The date features in the dataset are : \", date_features)\n",
    "    \n",
    "    for i in date_features:\n",
    "        print(\"Please specify whether you would like to split {} or convert it to julian format\".format(i))\n",
    "        print(\"Enter S to split, J to convert to Julian\")\n",
    "        split_or_julian_ind=input().upper()\n",
    "        if split_or_julian_ind == 'S':\n",
    "            data = date_splitter(data, fldname=i)\n",
    "        else:\n",
    "            data = Juliandtconv(data,fldname=i)\n",
    "            \n",
    "    date_transformed_data       = drop_columns(data,columnlist=date_features)\n",
    "    \n",
    "    data,target_is_separate_ind = concatenate_target(date_transformed_data,target)\n",
    "    \n",
    "                \n",
    "    print('Date tarnsformed data:\\n', data.head())\n",
    "    \n",
    "    input('Press enter to continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical features to numerical\n",
    "def encoding(df,categorical_features):\n",
    "    \n",
    "    df_cat = df[categorical_features]\n",
    "    #By defaault, convert all the categories to numerical forms using Label encoder\n",
    "    le = {}\n",
    "    for i in range(len(df_cat.columns.tolist())):\n",
    "        le[i] = LabelEncoder()\n",
    "        df_cat.iloc[:,i] = le[i].fit_transform(df_cat.iloc[:,i])\n",
    "\n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of categorical features in the dataset:\n",
      " ['DayOfWeek', 'PdDistrict', 'Address', 'Category']\n",
      "Encoding all categorical featues as numerical...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data looks like this: \n",
      "    DayOfWeek  PdDistrict  Address  Category           X          Y  \\\n",
      "0          6           4    19790        37 -122.425892  37.774599   \n",
      "1          6           4    19790        21 -122.425892  37.774599   \n",
      "2          6           4    22697        21 -122.424363  37.800414   \n",
      "3          6           4     4266        16 -122.426995  37.800873   \n",
      "4          6           5     1843        16 -122.438738  37.771541   \n",
      "\n",
      "    DatesJulian  \n",
      "0  2.457156e+06  \n",
      "1  2.457156e+06  \n",
      "2  2.457156e+06  \n",
      "3  2.457156e+06  \n",
      "4  2.457156e+06  \n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "if target_is_separate_ind == False:\n",
    "    continuous_features,categorical_features,date_features = separate_cont_and_cat_features(data)\n",
    "\n",
    "if categorical_features:\n",
    "    print('Here is the list of categorical features in the dataset:\\n', categorical_features)\n",
    "    print('Encoding all categorical featues as numerical...')\n",
    "    encoded_data  = encoding(data,categorical_features)\n",
    "    data          = pd.concat([encoded_data,data[continuous_features+date_features]],axis=1)\n",
    "    print('Encoded data looks like this: \\n',data.head())\n",
    "    input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bring all data into same scale\n",
    "def scale_data(df,scaling_type):  \n",
    "    \n",
    "    if scaling_type == 'MinMax':\n",
    "        scaler = MinMaxScaler()#scales based on minimum and maximum values in dataset\n",
    "        \n",
    "    if scaling_type == 'Standardize':\n",
    "        scaler = StandardScaler()#scales based on mean and standard deviation of dataset setting mean to 0 and standard deviation to 1\n",
    "        \n",
    "    if scaling_type == 'Robustscale':\n",
    "        scaler = RobustScaler()#scales based on interquartile range of data\n",
    "\n",
    "    return pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not target_is_separate_ind:\n",
    "    data,target,target_is_separate_ind                     = separate_target(data,target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the scaling method: MinMax/Standardize/Robustscale\n",
      "Invalid scaling method chosen. By default, data is scaled using Min-Max scaler\n",
      "Scaled data:\n",
      "    DayOfWeek  PdDistrict   Address         X         Y  DatesJulian  Category\n",
      "0        1.0    0.444444  0.852026  0.043578  0.001276     1.000000        37\n",
      "1        1.0    0.444444  0.852026  0.043578  0.001276     1.000000        21\n",
      "2        1.0    0.444444  0.977182  0.044337  0.001770     0.999997        21\n",
      "3        1.0    0.444444  0.183666  0.043030  0.001778     0.999996        16\n",
      "4        1.0    0.555556  0.079347  0.037198  0.001217     0.999996        16\n"
     ]
    }
   ],
   "source": [
    "scaling_method = input('Please choose the scaling method: MinMax/Standardize/Robustscale')\n",
    "\n",
    "if scaling_method in ['Standardize', 'Robustscale','MinMax']:\n",
    "    scale      = scaling_method\n",
    "else:\n",
    "    print('Invalid scaling method chosen. By default, data is scaled using Min-Max scaler')\n",
    "    scale      = 'MinMax'\n",
    "\n",
    "data,target_is_separate_ind       = concatenate_target(scale_data(data,scaling_type= scale),target)\n",
    "\n",
    "print('Scaled data:\\n',data.head())\n",
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize correlation between various features in dataset\n",
    "def halfHeatMap_corr(df, mirror=False,figsize_x=20, figsize_y=20):\n",
    "\n",
    "    # Create Correlation df\n",
    "    corr = df.corr()\n",
    "    # Plot figsize\n",
    "    fig, ax = plt.subplots(figsize=(figsize_x, figsize_y))\n",
    "    # Generate Color Map\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    if mirror == False:\n",
    "        # Drop self-correlations\n",
    "        dropSelf = np.zeros_like(corr)\n",
    "        dropSelf[np.triu_indices_from(dropSelf)] = True\n",
    "        \n",
    "    # Generate heatmap with mask on redundant values with a precision of 2 \n",
    "    sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\", mask=dropSelf)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop one of the two correlated features, the correlation coeffiecient is higher than the threshold\n",
    "def drop_highly_corr_features(df,threshold=0.95):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    #drop the column\n",
    "    df.drop(to_drop,axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function tio obtain a threshold value for outlier removal\n",
    "def get_outlier_threshold(df):\n",
    "        \n",
    "    Q1=df.quantile(0.25)#1st qaurtile \n",
    "    Q3=df.quantile(0.75)#3rd qaurtile\n",
    "    \n",
    "    IQR=Q3-Q1#Inter quartile range\n",
    "    #print(Q3+1.5*IQR)\n",
    "    \n",
    "    upper_threshold  = (Q3+1.5*IQR).mean()\n",
    "    lower_threshold  = (Q1-1.5*IQR).mean()\n",
    "        \n",
    "    return (upper_threshold,lower_threshold)#return thresholds as a tuple\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize outliers in the data\n",
    "def plotoutliers(df,target,collist=[],figsize_x=16,figsize_y=6):\n",
    "    \n",
    "    df,trgt=separate_target(df,target)#separate data and target\n",
    "    \n",
    "    #If the feature whose outliers are to be visualized, consider only those features else consider all continuous variables\n",
    "    if collist:\n",
    "        df=df[collist]\n",
    "    else:\n",
    "        df=df._get_numeric_data()\n",
    "\n",
    "    plt.figure(figsize=(figsize_x, figsize_y))#specify the size of the figure\n",
    "    ax=sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df),showmeans=True)#plot outliers\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)#rotate the labels so as to adjust space\n",
    "    \n",
    "    threshold = get_outlier_threshold(df)#obtain upper and lower threshold values \n",
    "   \n",
    "    ax.axhline(y=threshold[0], color='r',linestyle='--',label='Upper threshold')#plot upper threshold\n",
    "    ax.axhline(y=threshold[1], color='b',linestyle='--',label='Lower threshold')#plot lower threshold\n",
    "    ax.legend(loc='upper right')#set legend\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop rows which contain outliers\n",
    "def remove_outliers(df,column,threshold=(0,0),method='IQR'):\n",
    "    #Default method is to filter outliers on the basis of interquartile range as we need the data to be normally distributed to filter data based on standard deviation\n",
    "    \n",
    "    if method:\n",
    "\n",
    "        Q1      = df[column].quantile(0.25)#1st qaurtile of the feature\n",
    "        Q3      = df[column].quantile(0.75)#3rd quartile of the feature\n",
    "        IQR     = Q3 - Q1#interquartile range\n",
    "\n",
    "        df = df[~((df[column] < (Q1 - 1.5 * IQR)) |(df[column] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    if method =='threshold':\n",
    "        \n",
    "        df = df[~((df[column] < threshold[1]) |(df[column] > threshold[0]))]#remove data which is more than upper threshold and lesser than lower threshold\n",
    "        \n",
    "    if method == 'std':\n",
    "        \n",
    "        mean = df[column].mean()#mean of the feature\n",
    "        std  = df[column].std()#standard deviation of the feature\n",
    "        \n",
    "        df = df[df[column] < (3*std + mean)]\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and obtain feature importance\n",
    "def feature_imp(df,target,classification=True):\n",
    "    \n",
    "    df,trgt=separate_target(df,target)#separate target variable\n",
    "    \n",
    "    #Use random forest with 100 estimators to decide important features\n",
    "    if classification:\n",
    "        rnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 42)\n",
    "    else:\n",
    "        rnd_clf = RandomForestRegressor (n_estimators = 100 , criterion = 'entropy',random_state = 42)\n",
    "    \n",
    "    rnd_clf.fit(df,trgt)#fit the model\n",
    "    \n",
    "    #obtain feature importance along with columns\n",
    "    x, y = (list(x) for x in zip(*sorted(zip(rnd_clf.feature_importances_, df.columns), reverse = False)))\n",
    "    \n",
    "    fi_df=pd.DataFrame(x,index=y,columns=['Importance'])#create dataframe of feature importance\n",
    "    \n",
    "    #plot feature importance\n",
    "    plt.barh(np.arange(len(y)), x, align='center', alpha=0.5,color='g')\n",
    "    plt.yticks(np.arange(len(y)), y)\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    return fi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the list of important features by referring to feature importance plot and thereby setting threshold\n",
    "def get_imp_features(df,threshold):\n",
    "    \n",
    "    imp_features_df=df[(df['Importance']>threshold)].index.values.tolist()\n",
    "    \n",
    "    return imp_features_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize distribution of different levels of categorical variable \n",
    "def check_distribution(df,col=None):\n",
    "    pd.value_counts(df[col]).sort_values(ascending=False).plot(kind=\"bar\")#obtain count of different categories in descending order\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to reduce the number of features in the data\n",
    "def dimreduction(df,reqd_dim):\n",
    "    \n",
    "    components_required=min(reqd_dim,len(df.columns)) #if number of features < 30, then reduce dimension to to number of features else to 30\n",
    "    pca = PCA(n_components = components_required)\n",
    "    principalComponents = pca.fit_transform(df.iloc[:,:].values)#converting data to array so as to fit\n",
    "    principalDf = pd.DataFrame(data = principalComponents)#convert compressed data to dataframe\n",
    "    \n",
    "    return principalDf"
   ]
  }
 ],
 "metadata": {
  "datacleaner": {
   "position": {
    "top": "50px"
   },
   "python": {
    "varRefreshCmd": "try:\n    print(_datacleaner.dataframe_metadata())\nexcept:\n    print([])"
   },
   "window_display": false
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
