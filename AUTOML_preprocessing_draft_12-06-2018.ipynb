{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno #library to visualize missing data\n",
    "import re#library for regular expression\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler,Normalizer,StandardScaler,RobustScaler,LabelEncoder,OneHotEncoder \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read data to dataframe\n",
    "\n",
    "def read_file_to_df(filepath):\n",
    "    \n",
    "    df = pd.DataFrame()#define an empty dataframe\n",
    "    \n",
    "    try:\n",
    "        if filepath.endswith(\".csv\"):\n",
    "            df = pd.read_csv(filepath)\n",
    "            df_loaded_ind=True\n",
    "\n",
    "        elif (filepath.endswith(\".xls\") | filepath.endswith(\".xlsx\")):\n",
    "            df = pd.read_excel(filepath)\n",
    "            df_loaded_ind=True\n",
    "\n",
    "        else:\n",
    "            print('Invalid file type')\n",
    "            print('\\n')\n",
    "            df_loaded_ind = False\n",
    "    except:\n",
    "        print('Invalid file path')\n",
    "        print('\\n')\n",
    "        df_loaded_ind = False\n",
    "        \n",
    "                \n",
    "    return df_loaded_ind,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while(True):\n",
    "    filepath             = input('Please provide the file path for the data : ')\n",
    "    print('\\n')\n",
    "    data_loaded_ind,data = read_file_to_df(filepath)\n",
    "    if data_loaded_ind:\n",
    "        print('Data is successfully loaded!!')\n",
    "        break\n",
    "    else:\n",
    "        print('Error loading dataset!! Try again...')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Take a quick look at your data...\\n')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the ratio of null values in columns of dataset in descending order\n",
    "def get_null_count(df):\n",
    "   \n",
    "    na_count=df.isnull().sum().sort_values(ascending=False)/len(df)#columnwise ratio of null values\n",
    "    na_count=na_count[na_count>0]#choose only the columns whic have null values\n",
    "\n",
    "    return na_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize the null value distribution\n",
    "def visualize_null_count(df,null_count):\n",
    "\n",
    "    filtered_data = msno.nullity_filter(df,  filter='bottom',n=len(null_count))\n",
    "    msno.matrix(filtered_data,labels=True,figsize=(35, 15))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input('Press enter to continue ...')\n",
    "print('\\n')\n",
    "print('Checking for null values in the dataset...')\n",
    "print('\\n')\n",
    "if data.isnull().values.any():\n",
    "    null_count = get_null_count(data)\n",
    "    print('Below are the features that contain null values :\\n', null_count)\n",
    "    visualize_null_count(data,null_count)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('There is not any null value in the dataset!!')\n",
    "\n",
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The data types of every feature are as follows:\\n',data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop desired columns\n",
    "def drop_columns(df,columnlist=[]):\n",
    "    if columnlist:\n",
    "        df.drop(columnlist,axis=1,inplace=True)#Drop all the columns in the list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to validate column names\n",
    "def validate_columns(df,columnlist):\n",
    "    \n",
    "    invalid_columns = []\n",
    "    if columnlist:\n",
    "        invalid_columns = [col for col in columnlist if col not in df.columns.tolist()]\n",
    "                \n",
    "    return invalid_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_col_ind = (input('Would you like to drop any feature?-Y/N')).upper()\n",
    "print('\\n')\n",
    "if drop_col_ind == 'Y':\n",
    "    while(True):\n",
    "        drop_null_column_list = []\n",
    "        print('Enter the features you wish to drop, separated by comma :')\n",
    "        drop_null_column_list = [x for x in input().split(',')]\n",
    "        print('\\n')\n",
    "\n",
    "        if drop_null_column_list:\n",
    "            invalid_col_names = validate_columns(data,columnlist = drop_null_column_list)\n",
    "            if invalid_col_names:\n",
    "                print('Warning!! Some of the feature names you have entered are incorrect. List of incorrectly entered features are : ',invalid_col_names)\n",
    "                print('Please verify..')\n",
    "                print('\\n')\n",
    "            else:\n",
    "                break \n",
    "\n",
    "    data = drop_columns(data,columnlist=drop_null_column_list)\n",
    "    print('Specified features are successfully dropped...')\n",
    "    print('\\n')\n",
    "    print('Existing features in the dataset are: ', data.columns.tolist())\n",
    "    print('\\n')\n",
    "    input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to impute missing values\n",
    "def impute_missing(df, drop_nans):  \n",
    "\n",
    "    #drop the rows containing null values if drop_nans is set to True\n",
    "    if drop_nans:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        null_features = get_null_count(df).index.tolist()\n",
    "        print(\"Here is the list of features containing null values : \",null_features)\n",
    "        print(\"\\n\")\n",
    "        print(\"For each of these features, please specify how you would like to impute missing values,separated by commas\\n\")\n",
    "        print(\"You can enter 'mean' to replace missing values with mean of the feature, 'mode' to replace with mode or 'others' to replace with some other method\\n\")\n",
    "        imputation = [x for x in input().split(',')]\n",
    "        print(\"\\n\")\n",
    "        print(\"Imputing missing values...\")\n",
    "        print(\"\\n\") \n",
    "        for col,imp in zip(null_features,imputation):\n",
    "            if imp == 'mean':\n",
    "                column_median = df[col].median()\n",
    "                df[col].fillna(column_median, inplace=True)\n",
    "            elif imp == 'mode':\n",
    "                column_mode = df[col].mode()[0]\n",
    "                df[col].fillna(column_mode, inplace=True)\n",
    "            elif imp == 'others':\n",
    "                print(\"For {} :\".format(col))\n",
    "                imp_oth = input(\" Please specify the value to replace null values : \")\n",
    "                df[col].fillna(imp_oth, inplace=True)\n",
    "\n",
    "                \n",
    "    if df.isnull().values.any():\n",
    "        print('Error in replacing/dropping missing data')\n",
    "    else:\n",
    "        if drop_nans:\n",
    "            print('Missing values are successfully dropped')   \n",
    "        else:\n",
    "            print('Missing values are successfully replaced')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.isnull().values.any():\n",
    "    drop_na_rows = (input('Do you wish to drop all rows with null values?-Y/N')).upper()\n",
    "    print('\\n')\n",
    "    drop_nans = False\n",
    "    if drop_na_rows == 'Y':\n",
    "        drop_nans = True\n",
    "\n",
    "    data = impute_missing(data, drop_nans=drop_nans)\n",
    "    \n",
    "    print('\\n')\n",
    "    input('Press enter to continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = data.select_dtypes(include=['int64','int32','float64','float32'])\n",
    "int_columns = data_int.columns\n",
    "print('Plotting the features with integer data types..')\n",
    "no_cols = 5\n",
    "no_rows = int(np.ceil(len(int_columns) / no_cols))\n",
    "fig, axes = plt.subplots(ncols=no_cols,nrows=no_rows, figsize=(20,3*no_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(int_columns)):#(no_rows*no_cols):\n",
    "    axes[i].hist(data_int[int_columns[i]], bins=30,facecolor='r',alpha=0.5)\n",
    "    axes[i].set_title(int_columns[i])\n",
    "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
    "    plt.setp(axes[i].get_yticklabels(), visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert datatype of mentioned columns to corresponding mentioned types\n",
    "def data_type_converter(df,column_names=[],to_types=[]):\n",
    "    \n",
    "    if (isinstance(column_names, list) & isinstance(to_types, list)):#check if passed item is in list format\n",
    "\n",
    "        for col,typ in zip(column_names,to_types):\n",
    "            if typ == 'datetime64[ns]':\n",
    "                try:\n",
    "                    df[col] = df[col].astype(typ)\n",
    "                except:\n",
    "                    invalid_frmt=True\n",
    "                    while(invalid_frmt==True):\n",
    "                        try:\n",
    "                            print(\"Unable to convert {} to datetime format\".format(col))\n",
    "                            frmt=input('Please specify the datetime format of {} feature'.format(col))\n",
    "                            print(\"\\n\")\n",
    "                            try:\n",
    "                                df[col]=pd.to_datetime(data[col], format=frmt)\n",
    "                                invalid_frmt=False\n",
    "                            except:\n",
    "                                print(\"Oops!! Invalid format entered\")\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                df[col] = df[col].astype(typ)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please make sure that all the features are in int/float/string/category/boolean/timestamp format')\n",
    "print('\\n')\n",
    "print(\"Please note that accepted type for all date/timestamp feature conversion is : 'datetime64[ns]'\\n\")\n",
    "print(\"Example for date format: If your date is '2018-07-02-16.07.13.624406',format should be '%Y-%m-%d-%H.%M.%S.%f'\")\n",
    "print('\\n')\n",
    "type_change_ind = (input('Would you like to change the data type of any feature?-Y/N')).upper()\n",
    "print('\\n')\n",
    "if type_change_ind == 'Y':\n",
    "    while(True):\n",
    "        cols_for_type_change=[]\n",
    "        to_type_list=[]\n",
    "        print('Enter the features for which you wish to change the data types, separated by comma')\n",
    "        cols_for_type_change = [x for x in input().split(',')]\n",
    "        print('\\n')\n",
    "        invalid_col_names = validate_columns(data,columnlist = cols_for_type_change)\n",
    "        if invalid_col_names:\n",
    "            print('Warning!! Some of the feature names you have entered are incorrect. List of incorrectly entered features are : ',invalid_col_names)\n",
    "            print('Please verify..')\n",
    "            print('\\n')\n",
    "        else:\n",
    "            before_data_type = data[cols_for_type_change].dtypes.tolist()\n",
    "            break \n",
    "    print('Enter the type to which you would like to change the data types, in the same sequence as the features, separated by comma')\n",
    "    to_type_list         = [x for x in input().split(',')]\n",
    "    print('\\n')\n",
    "    data = data_type_converter(data,column_names=cols_for_type_change,to_types=to_type_list)\n",
    "    after_data_type = data[cols_for_type_change].dtypes.tolist()\n",
    "    print('The changed data types are as follows :\\n') \n",
    "    display(pd.DataFrame(list(zip(cols_for_type_change, before_data_type, after_data_type)),columns=['Features','Before', 'After']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to separate data and target variable \n",
    "def separate_target(df_raw,target_name):\n",
    "    \n",
    "    target             = df_raw[target_name]#target variable \n",
    "    df                 = df_raw.drop(target_name,axis=1)#data without target variable\n",
    "    target_is_separate = True\n",
    "      \n",
    "    return df,target,target_is_separate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_cont_and_cat_features(df):\n",
    "    \n",
    "    categorical_features = df.select_dtypes(include=['object','bool','category']).columns.tolist()\n",
    "    continuous_features  = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    date_features        = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "    \n",
    "    return continuous_features,categorical_features,date_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to splite timestamp/date column into sub features\n",
    "def date_splitter(df, fldname):\n",
    "    \n",
    "    #convert the specified feature to timestamp format if not already\n",
    "    fld = df[fldname] \n",
    "    #if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        #df[fldname] = fld = pd.to_datetime(fld,infer_datetime_format=True)\n",
    "        \n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)#substitute the phrase 'date' by spaces in the original field name\n",
    "    \n",
    "    #obtain different sub features for the date column\n",
    "    for n in ('Year', 'Month',  'Day', 'Hour' ,'Minute','Second'):\n",
    "        #if n in ['Hour' ,'Minute','Seconds']:\n",
    "        df[targ_pre+n] = getattr(fld.dt,n.lower())\n",
    "        if n in ['Hour' ,'Minute','Second']:\n",
    "            if any(df[targ_pre+n] > 0):\n",
    "                #df[targ_pre+n]=df[targ_pre+n].astype('str')    \n",
    "                df[targ_pre+n]=df[targ_pre+n].astype('category')\n",
    "            else:\n",
    "                df.drop(targ_pre+n,axis=1,inplace=True)\n",
    "        else:\n",
    "            df[targ_pre+n]=df[targ_pre+n].astype('str')    \n",
    "            df[targ_pre+n]=df[targ_pre+n].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert date field to its Julian format\n",
    "def Juliandtconv(df,fldname):\n",
    "    \n",
    "    #convert the specified feature to timestamp format if not already\n",
    "    #df[fldname]=pd.to_datetime(df[fldname],infer_datetime_format=True)\n",
    "    \n",
    "    df[fldname] = df[fldname].apply(lambda x: x.to_julian_date())#convert to julian format\n",
    "            \n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to concatenate data and target variable \n",
    "def concatenate_target(df,target):\n",
    "    target_is_separate=False\n",
    "    return (pd.concat([df,target],axis=1)),target_is_separate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name    = input('Please enter the target feature :')\n",
    "while(True):\n",
    "    invalid_target_name = validate_columns(data,columnlist = [target_name])\n",
    "    if invalid_target_name:\n",
    "        target_name = input('Please verify the target feature and re-enter :')\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "continuous_features,categorical_features,date_features = separate_cont_and_cat_features(data)\n",
    "\n",
    "if date_features:\n",
    "    print(\"Transforming date features in the data...\")\n",
    "    print('\\n')\n",
    "    print(\"The date features in the dataset are : \", date_features)\n",
    "    print('\\n')\n",
    "    \n",
    "    first_dt_feature=True\n",
    "    \n",
    "    for i in date_features:\n",
    "                  \n",
    "        \n",
    "        if i == target_name:\n",
    "            split_or_julian_ind=''\n",
    "            print(\"Please specify whether you would like to convert the target feature,{} to Julian format or retain as it is\".format(i))\n",
    "            while(split_or_julian_ind != 'J' and split_or_julian_ind != 'R'):\n",
    "                split_or_julian_ind=(input(\"Enter J to convert to Julian or R to retain existing date format : \")).upper()\n",
    "            print(\"\\n\")\n",
    "            if split_or_julian_ind == 'J':\n",
    "                data = Juliandtconv(data,fldname=i)\n",
    "            elif split_or_julian_ind == 'R':\n",
    "                pass\n",
    "        else:\n",
    "            if first_dt_feature:\n",
    "                first_dt_feature = False\n",
    "                allind=(input(\"Would you like to split all date feaures or convert all date features to Julian format? - Y/N\")).upper()\n",
    "                if allind == 'Y':\n",
    "                    split_or_julian_ind=''\n",
    "                    while(split_or_julian_ind != 'S' and split_or_julian_ind != 'J'):\n",
    "                        split_or_julian_ind=(input(\"Enter S to split, J to convert to Julian format : \")).upper()\n",
    "                        print(\"\\n\")\n",
    "            if allind != 'Y':  \n",
    "                #print(\"printing allind\",allind)\n",
    "                print(\"Please specify whether you would like to split {} or convert it to Julian format \".format(i))\n",
    "                split_or_julian_ind=''\n",
    "                while(split_or_julian_ind != 'S' and split_or_julian_ind != 'J'):\n",
    "                    split_or_julian_ind=(input(\"Enter S to split, J to convert to Julian format : \")).upper()\n",
    "                    print(\"\\n\")\n",
    "            if split_or_julian_ind == 'S':\n",
    "                data = date_splitter(data, fldname=i)\n",
    "                data.drop(i,axis=1,inplace=True)\n",
    "            elif split_or_julian_ind == 'J':\n",
    "                data = Juliandtconv(data,fldname=i)\n",
    "\n",
    "\n",
    "            \n",
    "       \n",
    "    #data,target_is_separate_ind = concatenate_target(data,target)\n",
    "    \n",
    "    print('\\n')            \n",
    "    print('Date tarnsformed data:\\n', data.head())\n",
    "    print('\\n')\n",
    "    input('Press enter to continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical features to numerical\n",
    "def encoding(df,categorical_features):\n",
    "    \n",
    "    df_cat = df[categorical_features]\n",
    "    #By defaault, convert all the categories to numerical forms using Label encoder\n",
    "    le = {}\n",
    "    for i in range(len(df_cat.columns.tolist())):\n",
    "        le[i] = LabelEncoder()\n",
    "        df_cat.iloc[:,i] = le[i].fit_transform(df_cat.iloc[:,i].astype(str))\n",
    "\n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if target_is_separate_ind:\n",
    "#    data,target_is_separate_ind       = concatenate_target(data,target)\n",
    "continuous_features,categorical_features,date_features = separate_cont_and_cat_features(data)\n",
    "\n",
    "if categorical_features:\n",
    "    print('Here is the list of categorical features in the dataset:\\n', categorical_features)\n",
    "    print('\\n')\n",
    "    print('Encoding all categorical featues as numerical...')\n",
    "    print('\\n')\n",
    "    encoded_data  = encoding(data,categorical_features)\n",
    "    data          = pd.concat([encoded_data,data[continuous_features+date_features]],axis=1)\n",
    "    print('Encoded data looks like this: \\n',data.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Press enter to see the description of your data')\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bring all data into same scale\n",
    "def scale_data(df,scaling_type):  \n",
    "    \n",
    "    if scaling_type == 'MinMax':\n",
    "        scaler = MinMaxScaler()#scales based on minimum and maximum values in dataset\n",
    "        \n",
    "    if scaling_type == 'Standardize':\n",
    "        scaler = StandardScaler()#scales based on mean and standard deviation of dataset setting mean to 0 and standard deviation to 1\n",
    "        \n",
    "    if scaling_type == 'Robustscale':\n",
    "        scaler = RobustScaler()#scales based on interquartile range of data\n",
    "\n",
    "    return pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_method = input('Please choose the scaling method: MinMax/Standardize/Robustscale')\n",
    "print('\\n')\n",
    "\n",
    "data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "\n",
    "if scaling_method in ['Standardize', 'Robustscale','MinMax']:\n",
    "    scale      = scaling_method\n",
    "else:\n",
    "    print('Invalid scaling method chosen. By default, data is scaled using Min-Max scaler')\n",
    "    print('\\n')\n",
    "    scale      = 'MinMax'\n",
    "\n",
    "data,target_is_separate_ind       = concatenate_target(scale_data(data,scaling_type= scale),target)\n",
    "\n",
    "print('Scaled data :\\n')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize correlation between various features in dataset\n",
    "def halfHeatMap_corr(df,figsize_x, figsize_y):\n",
    "\n",
    "    # Create Correlation df\n",
    "    corr = df.corr()\n",
    "    # Plot figsize\n",
    "    fig, ax = plt.subplots(figsize=(figsize_x, figsize_y))\n",
    "    # Generate Color Map\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Drop self-correlations\n",
    "    #dropSelf = np.zeros_like(corr)\n",
    "    #dropSelf[np.triu_indices_from(dropSelf)] = True\n",
    "    # Generate heatmap with mask on redundant values with a precision of 2 \n",
    "    sns.heatmap(corr, cmap=colormap)#, mask=dropSelf)\n",
    "\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "fig_size_x = 20\n",
    "fig_size_y = 20\n",
    "\n",
    "while(True):\n",
    "        print(\"Plotting correlation between the features...\")\n",
    "        print('\\n')\n",
    "\n",
    "        halfHeatMap_corr(data,figsize_x=fig_size_x, figsize_y=fig_size_y)\n",
    "\n",
    "        fig_size_change_ind = (input('Would you like to increase the plot size for a clear view? - Y/N')).upper()\n",
    "        print('\\n')\n",
    "\n",
    "        if fig_size_change_ind =='Y':\n",
    "            fig_size_x += 5\n",
    "            fig_size_y += 5\n",
    "        else:\n",
    "            break\n",
    "\n",
    "#input(\"Press enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "drop_corr_ind=(input('Would you like to drop any of the highly correlated features? - Y/N')).upper()\n",
    "print('\\n')\n",
    "if drop_corr_ind == 'Y':\n",
    "    corr_threshold_ind = (input(\"Would you like to specify a threshold of your choice-Y/N?..If not, default threshold value of 0.95 will be assumed.For every pair of correlated features,one of the correlated features above the threshold level will be dropped.\")).upper()\n",
    "    print('\\n')\n",
    "    if corr_threshold_ind == 'Y':\n",
    "        corr_threshold = float((input(\"Please refer to the correlation plot and specify a threshold of your choice.\")))\n",
    "        if (corr_threshold > 1 and corr_threshold < 0):\n",
    "            corr_threshold = 0.95   \n",
    "    else:\n",
    "        corr_threshold = 0.95\n",
    "    print('\\n')\n",
    "    \n",
    "    corr_matrix = data.corr().abs()\n",
    "    c1 = corr_matrix.stack().sort_values(ascending=False).drop_duplicates()\n",
    "    high_cor = c1[(c1.values!=1) & (c1.values > corr_threshold)]#\n",
    "    print(\"Below is the pair of highly correlated features excluding the target feature, whose correlation coefficient is greater than threshold:\\n \",high_cor)\n",
    "    print('\\n')\n",
    "    to_drop=[high_cor.index[i][0] for i in range(len(high_cor))]\n",
    "    print(\"Here is the list of features that will be dropped..: \",to_drop)\n",
    "    print('\\n')\n",
    "    \n",
    "    donot_drop_ind=(input(\"Is there any feature in the list that you would want to retain? - Y/N\")).upper()\n",
    "    donot_drop = []\n",
    "    if donot_drop_ind =='Y':\n",
    "        print(\"Enter the features that you would like to reatin seaparated by commas : \")\n",
    "        donot_drop=[x for x in input().split(',')]\n",
    "        \n",
    "    if donot_drop:\n",
    "        for i in donot_drop:\n",
    "            to_drop.remove(i)\n",
    "            \n",
    "    data.drop(to_drop,axis =1,inplace=True)\n",
    "    if to_drop:\n",
    "        print(\"List of features dropped is :\", to_drop)\n",
    "    else:\n",
    "        print(\"None of the features were dropped\")\n",
    "        \n",
    "print('\\n')  \n",
    "input(\"Press enter to continue...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function tio obtain a threshold value for outlier removal\n",
    "def get_outlier_threshold(df):\n",
    "        \n",
    "    Q1=df.quantile(0.25)#1st qaurtile \n",
    "    Q3=df.quantile(0.75)#3rd qaurtile\n",
    "    \n",
    "    IQR=Q3-Q1#Inter quartile range\n",
    "    #print(Q3+1.5*IQR)\n",
    "    \n",
    "    #upper_threshold  = (Q3+1.5*IQR).mean()\n",
    "    #lower_threshold  = (Q1-1.5*IQR).mean()\n",
    "    upper_whisker = df[df<=Q3+1.5*IQR].max()\n",
    "    lower_whisker = df[df>=Q1-1.5*IQR].min()\n",
    "    \n",
    "    upper_threshold_median  = upper_whisker.median()\n",
    "    upper_threshold_mean    = upper_whisker.mean()\n",
    "    lower_threshold_median  = lower_whisker.median()\n",
    "    lower_threshold_mean    = lower_whisker.mean()\n",
    "        \n",
    "    return (upper_threshold_median,lower_threshold_median,upper_threshold_mean,lower_threshold_mean)#return thresholds as a tuple\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize outliers in the data\n",
    "def plotoutliers(df,figsize_x=fig_size_x,figsize_y=fig_size_y):\n",
    "    \n",
    "    plt.figure(figsize=(figsize_x, figsize_y))#specify the size of the figure\n",
    "    ax=sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df),showmeans=True)#plot outliers\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)#rotate the labels so as to adjust space\n",
    "    \n",
    "    threshold = get_outlier_threshold(df)#obtain upper and lower threshold values \n",
    "   \n",
    "    ax.axhline(y=max(threshold[0],threshold[2]), color='r',linestyle='--',label='Upper threshold')#plot upper threshold\n",
    "    ax.axhline(y=min(threshold[1],threshold[3]), color='b',linestyle='--',label='Lower threshold')#plot lower threshold\n",
    "    ax.legend(loc='upper right')#set legend\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop rows which contain outliers\n",
    "def remove_outliers(df,column,threshold,method):\n",
    "    #Default method is to filter outliers on the basis of interquartile range as we need the data to be normally distributed to filter data based on standard deviation\n",
    "    \n",
    "    if method == 'IQR':\n",
    "\n",
    "        Q1      = df[column].quantile(0.25)#1st qaurtile of the feature\n",
    "        Q3      = df[column].quantile(0.75)#3rd quartile of the feature\n",
    "        IQR     = Q3 - Q1#interquartile range\n",
    "\n",
    "        df = df[~((df[column] < (Q1 - 1.5 * IQR)) |(df[column] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    if method =='threshold':\n",
    "        \n",
    "        df = df[~((df[column] < min(threshold[1],threshold[3])) |(df[column] > max(threshold[0],threshold[2])))]#remove data which is more than upper threshold and lesser than lower threshold\n",
    "        \n",
    "    if method == 'std':\n",
    "        \n",
    "        mean = df[column].mean()#mean of the feature\n",
    "        std  = df[column].std()#standard deviation of the feature\n",
    "        \n",
    "        df = df[df[column] < (3*std + mean)]\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig_size_x = 20\n",
    "fig_size_y = 20\n",
    "\n",
    "chck_outlier = True\n",
    "to_drop=[]\n",
    "while(chck_outlier == True):\n",
    "    \n",
    "    check_outlier = (input('Would you like to check for outliers in the data? - Y/N')).upper()\n",
    "    if check_outlier == 'Y':\n",
    "        \n",
    "        out_cont_features = [i for i in continuous_features  if i not in to_drop+ [target_name]]\n",
    "        out_cat_features  = [i for i in categorical_features if i not in to_drop+ [target_name]]\n",
    "\n",
    "        if out_cont_features:\n",
    "            loop_cont = True\n",
    "            while(loop_cont==True):\n",
    "                print(\"Plotting outliers for continuous features...\")\n",
    "                print('\\n')\n",
    "\n",
    "                #threshold = plotoutliers(data[out_cont_features],fig_size_x,fig_size_y)\n",
    "                threshold_cont = plotoutliers(data[out_cont_features],fig_size_x,fig_size_y)\n",
    "\n",
    "                fig_size_change_ind = (input('Would you like to increase the plot size for a clear view? - Y/N')).upper()\n",
    "                print('\\n')\n",
    "\n",
    "                if fig_size_change_ind =='Y':\n",
    "                    fig_size_x += 5\n",
    "                    fig_size_y += 5  \n",
    "                else:\n",
    "                    loop_cont = False\n",
    "\n",
    "        if out_cat_features:\n",
    "            loop_cat = True\n",
    "            while(loop_cat == True):\n",
    "                print(\"Plotting outliers for categorical features...\")\n",
    "                print('\\n')\n",
    "\n",
    "                #threshold = plotoutliers(data[out_cont_features],fig_size_x,fig_size_y)\n",
    "                threshold_cat = plotoutliers(data[out_cat_features],fig_size_x,fig_size_y)\n",
    "\n",
    "                fig_size_change_ind = (input('Would you like to increase the plot size for a clear view? - Y/N')).upper()\n",
    "                print('\\n')\n",
    "\n",
    "                if fig_size_change_ind =='Y':\n",
    "                    fig_size_x += 5\n",
    "                    fig_size_y += 5  \n",
    "                else:\n",
    "                    loop_cat = False\n",
    "\n",
    "        loop_all = True  \n",
    "        while(loop_all == True):\n",
    "            print(\"Plotting outliers for all features...\")\n",
    "            print('\\n')\n",
    "            if not target_is_separate_ind:\n",
    "                data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "            threshold_all = plotoutliers(data,fig_size_x,fig_size_y)\n",
    "\n",
    "            fig_size_change_ind = (input('Would you like to increase the plot size for a clear view? - Y/N')).upper()\n",
    "            print('\\n')\n",
    "\n",
    "            if fig_size_change_ind =='Y':\n",
    "                fig_size_x += 5\n",
    "                fig_size_y += 5\n",
    "            else:\n",
    "                loop_all = False\n",
    "\n",
    "        outlier_remove_ind =  (input(\"Would you like to remove any of the outliers? - Y/N\")).upper()\n",
    "        if outlier_remove_ind == 'Y':\n",
    "            if target_is_separate_ind:\n",
    "                data,target_is_separate_ind       = concatenate_target(data,target)\n",
    "            invalid_col=True\n",
    "            while(invalid_col==True):\n",
    "                col_for_outlier = input(\"Please enter the feature based on which you would like to remove outliers :\")\n",
    "                print(\"\\n\")\n",
    "                invalid_col_name = validate_columns(data,columnlist = [col_for_outlier])\n",
    "                if invalid_col_name:\n",
    "                    print('Warning!! Please verify the feature name and re-enter : ',invalid_col_name)\n",
    "                    print('\\n')\n",
    "                else:\n",
    "                    invalid_col=False\n",
    "            print(\"Please specify whether you would remove outliers based on Inter quartile range, threshold value or standard deviation\")\n",
    "            removal_method = input(\"Enter any of these : IQR / threshold / std\\n\")\n",
    "            print(\"\\n\")\n",
    "            outlier_thrshld = threshold_all\n",
    "            if removal_method == 'threshold':\n",
    "                print(\"Outliers will be removed based on the displayed threshold of all features unless you would want to change\")\n",
    "                threshld_cat_cont_ind = (input('Would you like to set the threshold based on only continuous or categorical features? - Y/N')).upper()\n",
    "                if threshld_cat_cont_ind == 'Y':\n",
    "                    cat_or_cont = (input('Enter A/B for threshold based on continuous/categorical features respectively')).upper()\n",
    "                    if cat_or_cont == 'A':\n",
    "                        outlier_thrshld = threshold_cont       \n",
    "                    elif cat_or_cont =='B':\n",
    "                        outlier_thrshld = threshold_cat\n",
    "                else:\n",
    "                    outlier_threshold_ind = input(\"Would you like to specify the threshold values for outlier removal? - Y/N\")\n",
    "                    print(\"\\n\")\n",
    "                    if outlier_threshold_ind == 'Y':\n",
    "                        lower_threshold = input(\"Enter the lower threshold :\")\n",
    "                        upper_threshold = input(\"Enter the upper threshold :\")\n",
    "                        print(\"\\n\")\n",
    "                        outlier_thrshld = (lower_threshold,upper_threshold)\n",
    "\n",
    "            data = remove_outliers(data,column = col_for_outlier,threshold = outlier_thrshld,method = removal_method)\n",
    "            print(\"Outliers are successfully removed..\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "             chck_outlier = False\n",
    "    else:\n",
    "         chck_outlier = False\n",
    "        \n",
    "\n",
    "    \n",
    "input(\"Press enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and obtain feature importance\n",
    "def feature_imp_plot(df,target,classification,figsize_x=10,figsize_y=20):\n",
    "    \n",
    "    #Use random forest with 100 estimators to decide important features\n",
    "    if classification:\n",
    "        rnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 42)\n",
    "    else:\n",
    "        rnd_clf = RandomForestRegressor (n_estimators = 100,random_state = 42)\n",
    "    \n",
    "    rnd_clf.fit(df,target)#fit the model\n",
    "    \n",
    "    #obtain feature importance along with columns\n",
    "    x, y = (list(x) for x in zip(*sorted(zip(rnd_clf.feature_importances_, df.columns), reverse = False)))\n",
    "    #x=[((i-min(x))/(max(x)-min(x)))for i in x]#scale the importances between 0 and 1 for better visualization\n",
    "    #x=[format(i,'.10f') for i in x]\n",
    "    fi_df=pd.DataFrame(x,index=y,columns=['Importance'])#create dataframe of feature importance\n",
    "    \n",
    "    #plot feature importance\n",
    "    plt.figure(figsize=(figsize_x, figsize_y))\n",
    "    ax=plt.barh(np.arange(len(y)), x, align='center', alpha=0.5,color='g',log=True)\n",
    "    ax=plt.yticks(np.arange(len(y)), y)\n",
    "    ax=plt.xlabel('Importance')\n",
    "    \n",
    "    #for p in ax.patches:\n",
    "        #plt.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    \n",
    "\n",
    "    \n",
    "    return fi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Plotting feature importance in log scale of importances...\")\n",
    "print('\\n')\n",
    "\n",
    "print(\"Please specify if this is a regression problem or classification\\n\")\n",
    "reg_or_class = (input(\"Enter R for Regression or C for Classification : \")).upper()\n",
    "\n",
    "if reg_or_class == 'C':\n",
    "    class_ind = True\n",
    "else:\n",
    "    class_ind = False\n",
    "\n",
    "if not target_is_separate_ind:\n",
    "    data,target,target_is_separate_ind                     = separate_target(data,target_name)\n",
    "\n",
    "data_fi = feature_imp_plot(data,target=target,classification=class_ind)\n",
    "plt.show()\n",
    "\n",
    "print(\"Below is the feature importance for every feature:\\n\",data_fi.sort_values('Importance', ascending= False))\n",
    "\n",
    "input(\"Press enter to continue...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the list of unimportant features by referring to feature importance plot and thereby setting threshold\n",
    "def get_unimp_features(df,threshold):\n",
    "    \n",
    "    unimp_features_list = df[(df['Importance'] < threshold)].index.values.tolist()\n",
    "    \n",
    "    return unimp_features_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features_ind = (input('Would you like to drop unimportant features? - Y/N')).upper()\n",
    "print('\\n')\n",
    "if drop_features_ind == 'Y':\n",
    "    fi_threshold = float(input('Please refer to the feature importance plot above and enter the threshold below which you would like to drop the features'))\n",
    "    print('\\n')\n",
    "    unimp_features = get_unimp_features(data_fi,fi_threshold)\n",
    "    print('Here is the list of unimportant features : ',unimp_features)\n",
    "    print('\\n')\n",
    "    print('Dropping  all unimportant features ...')\n",
    "    print('\\n')\n",
    "    data.drop(unimp_features,axis =1,inplace=True)\n",
    "    print('Now let us take a look at our dataset with important features only!!\\n ')\n",
    "    display(data.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to reduce the number of features in the data\n",
    "def dimreduction(df,reqd_dim):\n",
    "    \n",
    "    components_required=min(reqd_dim,len(df.columns)) #if number of features < 30, then reduce dimension to to number of features else to 30\n",
    "    pca = PCA(n_components = components_required)\n",
    "    principalComponents = pca.fit_transform(df.iloc[:,:].values)#converting data to array so as to fit\n",
    "    principalDf = pd.DataFrame(data = principalComponents,index=df.index)#convert compressed data to dataframe\n",
    "    \n",
    "    return principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red_ind = (input('Would you like to reduce dimensions of the data? - Y/N')).upper()\n",
    "if dim_red_ind == 'Y':\n",
    "    no_of_comp = int(input('Please enter the number of dimensions that you would like to reduce to :'))\n",
    "    data = dimreduction(data,reqd_dim = no_of_comp)\n",
    "    print('Here is your compressed data..\\n')\n",
    "    display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target_is_separate_ind       = concatenate_target(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datacleaner": {
   "position": {
    "top": "50px"
   },
   "python": {
    "varRefreshCmd": "try:\n    print(_datacleaner.dataframe_metadata())\nexcept:\n    print([])"
   },
   "window_display": false
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
